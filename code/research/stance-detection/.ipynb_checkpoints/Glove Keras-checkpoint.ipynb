{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stance Detection using Glove Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRELOAD = True\n",
    "preload_path = \"data_dump_glove.data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PRELOAD == True:\n",
    "    import pickle\n",
    "    data = pickle.load(open(preload_path, \"rb\"))\n",
    "    data_train = data[\"X_train\"]\n",
    "    labels_train = data[\"Y_train\"]\n",
    "    data_test = data[\"X_test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PRELOAD == False:\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    dtypes_train = {\"id\":np.int64, \"text\":str, \"author\":str, \"title\":str, \"label\":np.int64}\n",
    "    dtypes_test = {\"id\":np.int64, \"text\":str, \"author\":str, \"title\":str}\n",
    "\n",
    "    SEED = 1234\n",
    "\n",
    "    # load data\n",
    "\n",
    "    train_df = pd.read_csv(\"data/train.csv\", dtype=dtypes_train)\n",
    "    train_df = train_df.dropna()\n",
    "#     train_df = train_df.sample(frac=1)\n",
    "    X_train = train_df.drop(['label', 'id', 'author'], axis=1).values\n",
    "    Y_train = train_df['label'].values\n",
    "    print(\"Train DIMS \\nX dims: {} Y dims: {}\".format(X_train.shape, Y_train.shape))\n",
    "\n",
    "    test_df = pd.read_csv(\"data/test.csv\")\n",
    "    test_df = test_df.dropna()\n",
    "    X_test = test_df.drop(['id', 'author'], axis=1).values\n",
    "    print(\"Test DIMS \\nX dims: {}\".format(X_test.shape))\n",
    "    print(\"Num Labels: \", np.unique(Y_train))\n",
    "    \n",
    "    import pickle\n",
    "    from nltk.corpus import stopwords\n",
    "    import re\n",
    "    \n",
    "    # preprocessing\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    \n",
    "\n",
    "    for i in range(len(X_train)):\n",
    "        headline, article = X_train[i]\n",
    "        \n",
    "        headline = re.sub(r'[^\\w\\s]|\\n|\\r','',headline)\n",
    "        article = re.sub(r'[^\\w\\s]|\\n|\\r','',article)\n",
    "        \n",
    "        headline = headline.lower().split(\" \")\n",
    "        article = article.lower().split(\" \")\n",
    "        \n",
    "        headline = [word for word in headline if word not in stop_words]\n",
    "        article = [word for word in article if word not in stop_words]\n",
    "        \n",
    "        headline = \" \".join(headline)\n",
    "        article = \" \".join(article)\n",
    "        \n",
    "        train_data.append([ headline, article])\n",
    "\n",
    "    for i in range(len(X_test)):\n",
    "        headline, article = X_test[i]\n",
    "        \n",
    "        headline = re.sub(r'[^\\w\\s]|\\n|\\r','',headline)\n",
    "        article = re.sub(r'[^\\w\\s]|\\n|\\r','',article)\n",
    "        \n",
    "        headline = headline.lower().split(\" \")\n",
    "        article = article.lower().split(\" \")\n",
    "        \n",
    "        headline = [word for word in headline if word not in stop_words]\n",
    "        article = [word for word in article if word not in stop_words]\n",
    "        \n",
    "        headline = \" \".join(headline)\n",
    "        article = \" \".join(article)\n",
    "        \n",
    "        test_data.append([ headline, article])\n",
    "\n",
    "\n",
    "    data_save = {\n",
    "            \"X_train\": train_data, \n",
    "            \"Y_train\": Y_train,\n",
    "            \"X_test\": test_data,\n",
    "    }\n",
    "    pickle.dump(data_save, open(\"data_dump_glove.data\", \"wb\"))\n",
    "    print(\"Saved pre-preocessed data as : {}\".format(\"data_dump_glove.data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path :  c:\\users\\joavi\\anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\_pywrap_tensorflow_internal.pyd \n",
      "name :  _pywrap_tensorflow_internal\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100th sample\n",
      "\n",
      "headline : snap shares leap 44 debut investors doubt value vanish  new york times\n",
      "\n",
      "article : snapchat business built large part disappearing messages adding animated dog ears flower crowns users selfies thursday business worth 34 billion     market value   media company cbs three times size another social media company twitter snapchat made paper billionaires   founders five times making stock market debut spectacular fashion     shares rising 44 percent first day trading     snapchats parent snap inc blazed trail technology darlings like uber spotify remain privately held elated wall street institutions eager prominent initial public offering surfaced months company entranced investors despite litany red flags like enormous losses expected persist years slowdown   user growth rates ownership structure gives snapchats founders control decades come shadows onetime tech highfliers since crashed earth twitter valued nearly 32 billion end first day trading wall street values roughly 11 billion called company sell earlier force social media myspace sold rupert murdochs news corporation 580 million 2005 six years later sold justin timberlake investors 35 million analysts already shown skepticism newest publicly traded tech giant one brian wieser pivotal research group says share price 10 far companys offering price 17 initial range predicted snap faces competition larger companies challenge   user base said boosters snaps prospects argue instead snap potential become less like twitter like biggest rival 395 billion facebook supporters point companys obvious strengths 158 million people average used snapchat day end 2016 roughly 18 times day users opened app average 404 million sales collected last year nothing three years ago investors appeared focus positive snap raised 3 4 billion market debut american tech company since facebooks initial offering 2012 according data renaissance capital first significant tech stock sale since least december 44 percent pop stock price biggest enjoyed company   p since twitters debut 2013 217 million shares traded thursday investors bought others cashed exceeding number shares snap sold p investors   unicorns     term   valued 1 billion     immense success snaps deal highlights appetite tech darling even company still bleeds money count uber spotify airbnb within group sound youre hearing today snap p happy snapping fingers   unicorns investors said kathleen smith principal renaissance capital looks like snap set path monetization  countless meetings   roadshow investors snap executives sought rebut biggest concerns companys prospects slowing growth toward end last year stemmed problems services android app competition facebook openly copied snapchats signature features instagram would little dent user enthusiasm company would continue press innovations branded lens filters transform users monsters fairies taco bell tacos become new forms advertising beloved brands potential new ideas include drones   cameras wednesday night snaps bankers drawn list investors would get first shares largely big mutual funds hedge funds aim picking firms likely stick around long term p minted wealth others invested younger company including big venture capital firms like benchmark capital high school bay area unlike newly public companies seek celebrate first day trading stock markets snap kept festivities largely confined new york stock exchange companys top executives board members gathered   breakfast guests presented pins shape snapchats ghost mascot evan spiegel 26 bobby murphy 28 snaps founders briefly addressed crowd uncharacteristically clad suits ties rather customary   also presented exchange officials version customary gift given market debutants one vending machines sell companys spectacles   sunglasses send   videos app machine wont refilled sells spectacles founders walked floor exchange bedecked companys signature yellow     color splayed electronic boards wrapped around water bottles leaping executives ties one snap employee ducked hermès store street pick yellow tie days occasion mr spiegels fiancée supermodel miranda kerr enthusiastically documented day snapchat story posed selfies attendees floor could unlock special filter placed companys ghost mascot videos holding virtual snapchat balloons ejecting rainbow mouth 930 mr spiegel mr murphy rang big boards opening bell briefly basked adulation snaps chief strategy officer imran khan escorted family around exchange posed pictures fellow employees time snaps shares opened trading 24     later morning     companys top executives disappeared floor heading nearby offices one banks involved public offering goldman sachs watch opening many staff members trekked snaps midtown manhattan offices head back work\n",
      "\n",
      "label : 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"100th sample\\n\")\n",
    "print(\"headline : {}\\n\".format(data_train[100][0]))\n",
    "print(\"article : {}\\n\".format(data_train[100][1]))\n",
    "print(\"label : {}\\n\".format(labels_train[100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_train, labels_train, test_size=0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_headlines = [i[0] for i in X_train]\n",
    "X_train_articles = [i[1] for i in X_train]\n",
    "X_test_headlines = [i[0] for i in X_test]\n",
    "X_test_articles = [i[1] for i in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "# fit headline\n",
    "tokenizer.fit_on_texts(X_train_headlines + X_train_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_headlines_sequence = tokenizer.texts_to_sequences(X_train_headlines)\n",
    "X_train_articles_sequence = tokenizer.texts_to_sequences(X_train_articles)\n",
    "\n",
    "X_test_headlines_sequence = tokenizer.texts_to_sequences(X_test_headlines)\n",
    "X_test_articles_sequence = tokenizer.texts_to_sequences(X_test_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding\n",
    "max_words_headline = 70\n",
    "max_words_article = 1000\n",
    "\n",
    "X_train_headlines_sequence = sequence.pad_sequences(X_train_headlines_sequence, maxlen=max_words_headline)\n",
    "X_train_articles_sequence = sequence.pad_sequences(X_train_articles_sequence, maxlen=max_words_article)\n",
    "\n",
    "X_test_headlines_sequence = sequence.pad_sequences(X_test_headlines_sequence, maxlen=max_words_headline)\n",
    "X_test_articles_sequence = sequence.pad_sequences(X_test_articles_sequence, maxlen=max_words_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1193514it [00:43, 27161.24it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings_path = 'glove.twitter.27B.100d.txt'\n",
    "\n",
    "embeddings_index = dict()\n",
    "f = open(embeddings_path, encoding=\"utf8\")\n",
    "for line in tqdm(f):\n",
    "    values = line.split(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_word_vectors = 25000\n",
    "embed_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embs = np.stack(embeddings_index.values())\n",
    "embedding_matrix = np.random.normal(all_embs.mean(), all_embs.std(), \n",
    "                                        (max_word_vectors, embed_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in tokenizer.word_index.items():\n",
    "        if i >= max_word_vectors:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Input, Model\n",
    "from keras.layers import Embedding, LSTM, concatenate, Dense\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_input = Input(shape=(max_words_article,))\n",
    "article_emedd = Embedding(max_word_vectors, embed_dim, input_length=max_words_article,\n",
    "                       weights=[embedding_matrix], trainable=False)(article_input)\n",
    "article_lstm = LSTM(100, dropout=0.2, recurrent_dropout=0.2)(article_emedd)\n",
    "article_dense = Dense(100, activation='relu')(article_lstm)\n",
    "\n",
    "\n",
    "headline_input = Input(shape=(max_words_headline,))\n",
    "headline_emedd = Embedding(max_word_vectors, embed_dim, input_length=max_words_headline,\n",
    "                       weights=[embedding_matrix], trainable=False)(headline_input)\n",
    "headline_lstm = LSTM(100, dropout=0.2, recurrent_dropout=0.2)(headline_emedd)\n",
    "headline_dense = Dense(100, activation='relu')(headline_lstm)\n",
    "\n",
    "concat_dense = concatenate(inputs=[headline_dense, article_dense])\n",
    "output = Dense(1, activation='sigmoid')(concat_dense)\n",
    "\n",
    "model = Model(inputs=[headline_input, article_input], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 70)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 1000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 70, 100)      2500000     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1000, 100)    2500000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 100)          80400       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 100)          80400       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 100)          10100       lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          10100       lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 200)          0           dense_2[0][0]                    \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            201         concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 5,181,201\n",
      "Trainable params: 181,201\n",
      "Non-trainable params: 5,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11702 samples, validate on 2926 samples\n",
      "Epoch 1/1\n",
      "11702/11702 [==============================] - ETA: 1:25 - loss: 0.2314 - acc: 0.912 - ETA: 1:22 - loss: 0.2347 - acc: 0.903 - ETA: 1:18 - loss: 0.2276 - acc: 0.907 - ETA: 1:15 - loss: 0.2236 - acc: 0.912 - ETA: 1:11 - loss: 0.2274 - acc: 0.911 - ETA: 1:07 - loss: 0.2252 - acc: 0.912 - ETA: 1:03 - loss: 0.2270 - acc: 0.911 - ETA: 59s - loss: 0.2235 - acc: 0.913 - ETA: 55s - loss: 0.2249 - acc: 0.91 - ETA: 51s - loss: 0.2234 - acc: 0.91 - ETA: 47s - loss: 0.2224 - acc: 0.91 - ETA: 43s - loss: 0.2212 - acc: 0.91 - ETA: 39s - loss: 0.2205 - acc: 0.91 - ETA: 35s - loss: 0.2174 - acc: 0.91 - ETA: 31s - loss: 0.2137 - acc: 0.91 - ETA: 27s - loss: 0.2094 - acc: 0.91 - ETA: 23s - loss: 0.2078 - acc: 0.92 - ETA: 19s - loss: 0.2059 - acc: 0.92 - ETA: 15s - loss: 0.2047 - acc: 0.92 - ETA: 11s - loss: 0.2030 - acc: 0.92 - ETA: 7s - loss: 0.2012 - acc: 0.9226 - ETA: 3s - loss: 0.2005 - acc: 0.922 - 99s 8ms/step - loss: 0.1992 - acc: 0.9237 - val_loss: 0.1528 - val_acc: 0.9405\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x195644ebd30>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "            x=[X_train_headlines_sequence, X_train_articles_sequence], \n",
    "            y=y_train,\n",
    "            validation_split=0.2,\n",
    "            callbacks=[\n",
    "                ModelCheckpoint(filepath=\"glove100d.hdf5\", monitor='val_loss', save_best_only=True),\n",
    "                ReduceLROnPlateau(patience=1)\n",
    "            ],\n",
    "            verbose=1,\n",
    "            shuffle=True,\n",
    "            batch_size=512,\n",
    "            epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
