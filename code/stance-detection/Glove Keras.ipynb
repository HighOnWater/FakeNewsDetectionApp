{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stance Detection using Glove Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRELOAD = True\n",
    "preload_path = \"data_dump_glove.data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PRELOAD == True:\n",
    "    import pickle\n",
    "    data = pickle.load(open(preload_path, \"rb\"))\n",
    "    data_train = data[\"X_train\"]\n",
    "    labels_train = data[\"Y_train\"]\n",
    "    data_test = data[\"X_test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PRELOAD == False:\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    dtypes_train = {\"id\":np.int64, \"text\":str, \"author\":str, \"title\":str, \"label\":np.int64}\n",
    "    dtypes_test = {\"id\":np.int64, \"text\":str, \"author\":str, \"title\":str}\n",
    "\n",
    "    SEED = 1234\n",
    "\n",
    "    # load data\n",
    "\n",
    "    train_df = pd.read_csv(\"data/train.csv\", dtype=dtypes_train)\n",
    "    train_df = train_df.dropna()\n",
    "#     train_df = train_df.sample(frac=1)\n",
    "    X_train = train_df.drop(['label', 'id', 'author'], axis=1).values\n",
    "    Y_train = train_df['label'].values\n",
    "    print(\"Train DIMS \\nX dims: {} Y dims: {}\".format(X_train.shape, Y_train.shape))\n",
    "\n",
    "    test_df = pd.read_csv(\"data/test.csv\")\n",
    "    test_df = test_df.dropna()\n",
    "    X_test = test_df.drop(['id', 'author'], axis=1).values\n",
    "    print(\"Test DIMS \\nX dims: {}\".format(X_test.shape))\n",
    "    print(\"Num Labels: \", np.unique(Y_train))\n",
    "    \n",
    "    import pickle\n",
    "    from nltk.corpus import stopwords\n",
    "    import re\n",
    "    \n",
    "    # preprocessing\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    \n",
    "\n",
    "    for i in range(len(X_train)):\n",
    "        headline, article = X_train[i]\n",
    "        \n",
    "        headline = re.sub(r'[^\\w\\s]|\\n|\\r','',headline)\n",
    "        article = re.sub(r'[^\\w\\s]|\\n|\\r','',article)\n",
    "        \n",
    "        headline = headline.lower().split(\" \")\n",
    "        article = article.lower().split(\" \")\n",
    "        \n",
    "        headline = [word for word in headline if word not in stop_words]\n",
    "        article = [word for word in article if word not in stop_words]\n",
    "        \n",
    "        headline = \" \".join(headline)\n",
    "        article = \" \".join(article)\n",
    "        \n",
    "        train_data.append([ headline, article])\n",
    "\n",
    "    for i in range(len(X_test)):\n",
    "        headline, article = X_test[i]\n",
    "        \n",
    "        headline = re.sub(r'[^\\w\\s]|\\n|\\r','',headline)\n",
    "        article = re.sub(r'[^\\w\\s]|\\n|\\r','',article)\n",
    "        \n",
    "        headline = headline.lower().split(\" \")\n",
    "        article = article.lower().split(\" \")\n",
    "        \n",
    "        headline = [word for word in headline if word not in stop_words]\n",
    "        article = [word for word in article if word not in stop_words]\n",
    "        \n",
    "        headline = \" \".join(headline)\n",
    "        article = \" \".join(article)\n",
    "        \n",
    "        test_data.append([ headline, article])\n",
    "\n",
    "\n",
    "    data_save = {\n",
    "            \"X_train\": train_data, \n",
    "            \"Y_train\": Y_train,\n",
    "            \"X_test\": test_data,\n",
    "    }\n",
    "    pickle.dump(data_save, open(\"data_dump_glove.data\", \"wb\"))\n",
    "    print(\"Saved pre-preocessed data as : {}\".format(\"data_dump_glove.data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joavi\\Anaconda3\\envs\\py36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path :  C:\\Users\\joavi\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\_pywrap_tensorflow_internal.pyd \n",
      "name :  _pywrap_tensorflow_internal\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100th sample\n",
      "\n",
      "headline : snap shares leap 44 debut investors doubt value vanish  new york times\n",
      "\n",
      "article : snapchat business built large part disappearing messages adding animated dog ears flower crowns users selfies thursday business worth 34 billion     market value   media company cbs three times size another social media company twitter snapchat made paper billionaires   founders five times making stock market debut spectacular fashion     shares rising 44 percent first day trading     snapchats parent snap inc blazed trail technology darlings like uber spotify remain privately held elated wall street institutions eager prominent initial public offering surfaced months company entranced investors despite litany red flags like enormous losses expected persist years slowdown   user growth rates ownership structure gives snapchats founders control decades come shadows onetime tech highfliers since crashed earth twitter valued nearly 32 billion end first day trading wall street values roughly 11 billion called company sell earlier force social media myspace sold rupert murdochs news corporation 580 million 2005 six years later sold justin timberlake investors 35 million analysts already shown skepticism newest publicly traded tech giant one brian wieser pivotal research group says share price 10 far companys offering price 17 initial range predicted snap faces competition larger companies challenge   user base said boosters snaps prospects argue instead snap potential become less like twitter like biggest rival 395 billion facebook supporters point companys obvious strengths 158 million people average used snapchat day end 2016 roughly 18 times day users opened app average 404 million sales collected last year nothing three years ago investors appeared focus positive snap raised 3 4 billion market debut american tech company since facebooks initial offering 2012 according data renaissance capital first significant tech stock sale since least december 44 percent pop stock price biggest enjoyed company   p since twitters debut 2013 217 million shares traded thursday investors bought others cashed exceeding number shares snap sold p investors   unicorns     term   valued 1 billion     immense success snaps deal highlights appetite tech darling even company still bleeds money count uber spotify airbnb within group sound youre hearing today snap p happy snapping fingers   unicorns investors said kathleen smith principal renaissance capital looks like snap set path monetization  countless meetings   roadshow investors snap executives sought rebut biggest concerns companys prospects slowing growth toward end last year stemmed problems services android app competition facebook openly copied snapchats signature features instagram would little dent user enthusiasm company would continue press innovations branded lens filters transform users monsters fairies taco bell tacos become new forms advertising beloved brands potential new ideas include drones   cameras wednesday night snaps bankers drawn list investors would get first shares largely big mutual funds hedge funds aim picking firms likely stick around long term p minted wealth others invested younger company including big venture capital firms like benchmark capital high school bay area unlike newly public companies seek celebrate first day trading stock markets snap kept festivities largely confined new york stock exchange companys top executives board members gathered   breakfast guests presented pins shape snapchats ghost mascot evan spiegel 26 bobby murphy 28 snaps founders briefly addressed crowd uncharacteristically clad suits ties rather customary   also presented exchange officials version customary gift given market debutants one vending machines sell companys spectacles   sunglasses send   videos app machine wont refilled sells spectacles founders walked floor exchange bedecked companys signature yellow     color splayed electronic boards wrapped around water bottles leaping executives ties one snap employee ducked hermès store street pick yellow tie days occasion mr spiegels fiancée supermodel miranda kerr enthusiastically documented day snapchat story posed selfies attendees floor could unlock special filter placed companys ghost mascot videos holding virtual snapchat balloons ejecting rainbow mouth 930 mr spiegel mr murphy rang big boards opening bell briefly basked adulation snaps chief strategy officer imran khan escorted family around exchange posed pictures fellow employees time snaps shares opened trading 24     later morning     companys top executives disappeared floor heading nearby offices one banks involved public offering goldman sachs watch opening many staff members trekked snaps midtown manhattan offices head back work\n",
      "\n",
      "label : 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"100th sample\\n\")\n",
    "print(\"headline : {}\\n\".format(data_train[100][0]))\n",
    "print(\"article : {}\\n\".format(data_train[100][1]))\n",
    "print(\"label : {}\\n\".format(labels_train[100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_train, labels_train, test_size=0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_headlines = [i[0] for i in X_train]\n",
    "X_train_articles = [i[1] for i in X_train]\n",
    "X_test_headlines = [i[0] for i in X_test]\n",
    "X_test_articles = [i[1] for i in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "# fit headline\n",
    "tokenizer.fit_on_texts(X_train_headlines + X_train_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_headlines_sequence = tokenizer.texts_to_sequences(X_train_headlines)\n",
    "X_train_articles_sequence = tokenizer.texts_to_sequences(X_train_articles)\n",
    "\n",
    "X_test_headlines_sequence = tokenizer.texts_to_sequences(X_test_headlines)\n",
    "X_test_articles_sequence = tokenizer.texts_to_sequences(X_test_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding\n",
    "max_words_headline = 70\n",
    "max_words_article = 1000\n",
    "\n",
    "X_train_headlines_sequence = sequence.pad_sequences(X_train_headlines_sequence, maxlen=max_words_headline)\n",
    "X_train_articles_sequence = sequence.pad_sequences(X_train_articles_sequence, maxlen=max_words_article)\n",
    "\n",
    "X_test_headlines_sequence = sequence.pad_sequences(X_test_headlines_sequence, maxlen=max_words_headline)\n",
    "X_test_articles_sequence = sequence.pad_sequences(X_test_articles_sequence, maxlen=max_words_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1193514it [00:43, 27161.24it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings_path = 'glove.twitter.27B.100d.txt'\n",
    "\n",
    "embeddings_index = dict()\n",
    "f = open(embeddings_path, encoding=\"utf8\")\n",
    "for line in tqdm(f):\n",
    "    values = line.split(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_word_vectors = 25000\n",
    "embed_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embs = np.stack(embeddings_index.values())\n",
    "embedding_matrix = np.random.normal(all_embs.mean(), all_embs.std(), \n",
    "                                        (max_word_vectors, embed_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in tokenizer.word_index.items():\n",
    "        if i >= max_word_vectors:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Input, Model\n",
    "from keras.layers import Embedding, LSTM, concatenate, Dense\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_input = Input(shape=(max_words_article,))\n",
    "article_emedd = Embedding(max_word_vectors, embed_dim, input_length=max_words_article,\n",
    "                       weights=[embedding_matrix], trainable=False)(article_input)\n",
    "article_lstm = LSTM(100, dropout=0.2, recurrent_dropout=0.2)(article_emedd)\n",
    "article_dense = Dense(100, activation='relu')(article_lstm)\n",
    "\n",
    "\n",
    "headline_input = Input(shape=(max_words_headline,))\n",
    "headline_emedd = Embedding(max_word_vectors, embed_dim, input_length=max_words_headline,\n",
    "                       weights=[embedding_matrix], trainable=False)(headline_input)\n",
    "headline_lstm = LSTM(100, dropout=0.2, recurrent_dropout=0.2)(headline_emedd)\n",
    "headline_dense = Dense(100, activation='relu')(headline_lstm)\n",
    "\n",
    "concat_dense = concatenate(inputs=[headline_dense, article_dense])\n",
    "output = Dense(1, activation='sigmoid')(concat_dense)\n",
    "\n",
    "model = Model(inputs=[headline_input, article_input], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 70)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 1000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 70, 100)      2500000     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1000, 100)    2500000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 100)          80400       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 100)          80400       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 100)          10100       lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          10100       lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 200)          0           dense_2[0][0]                    \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            201         concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 5,181,201\n",
      "Trainable params: 181,201\n",
      "Non-trainable params: 5,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11702 samples, validate on 2926 samples\n",
      "Epoch 1/20\n",
      "11702/11702 [==============================] - ETA: 1:40 - loss: 0.1735 - acc: 0.927 - ETA: 1:30 - loss: 0.1614 - acc: 0.935 - ETA: 1:24 - loss: 0.1551 - acc: 0.939 - ETA: 1:19 - loss: 0.1520 - acc: 0.941 - ETA: 1:15 - loss: 0.1497 - acc: 0.943 - ETA: 1:10 - loss: 0.1565 - acc: 0.939 - ETA: 1:06 - loss: 0.1564 - acc: 0.940 - ETA: 1:02 - loss: 0.1579 - acc: 0.940 - ETA: 57s - loss: 0.1582 - acc: 0.939 - ETA: 53s - loss: 0.1602 - acc: 0.93 - ETA: 48s - loss: 0.1604 - acc: 0.93 - ETA: 44s - loss: 0.1608 - acc: 0.93 - ETA: 40s - loss: 0.1585 - acc: 0.93 - ETA: 36s - loss: 0.1570 - acc: 0.93 - ETA: 32s - loss: 0.1563 - acc: 0.93 - ETA: 28s - loss: 0.1558 - acc: 0.93 - ETA: 24s - loss: 0.1568 - acc: 0.93 - ETA: 19s - loss: 0.1586 - acc: 0.93 - ETA: 15s - loss: 0.1603 - acc: 0.93 - ETA: 11s - loss: 0.1618 - acc: 0.93 - ETA: 7s - loss: 0.1613 - acc: 0.9378 - ETA: 3s - loss: 0.1619 - acc: 0.937 - 101s 9ms/step - loss: 0.1611 - acc: 0.9379 - val_loss: 0.1341 - val_acc: 0.9460\n",
      "Epoch 2/20\n",
      "11702/11702 [==============================] - ETA: 1:28 - loss: 0.1132 - acc: 0.966 - ETA: 1:23 - loss: 0.1324 - acc: 0.956 - ETA: 1:20 - loss: 0.1366 - acc: 0.951 - ETA: 1:16 - loss: 0.1384 - acc: 0.951 - ETA: 1:12 - loss: 0.1353 - acc: 0.952 - ETA: 1:08 - loss: 0.1353 - acc: 0.951 - ETA: 1:04 - loss: 0.1351 - acc: 0.951 - ETA: 1:00 - loss: 0.1388 - acc: 0.950 - ETA: 56s - loss: 0.1390 - acc: 0.949 - ETA: 52s - loss: 0.1410 - acc: 0.94 - ETA: 48s - loss: 0.1408 - acc: 0.94 - ETA: 44s - loss: 0.1424 - acc: 0.94 - ETA: 40s - loss: 0.1450 - acc: 0.94 - ETA: 36s - loss: 0.1459 - acc: 0.94 - ETA: 32s - loss: 0.1470 - acc: 0.94 - ETA: 28s - loss: 0.1503 - acc: 0.94 - ETA: 23s - loss: 0.1490 - acc: 0.94 - ETA: 19s - loss: 0.1490 - acc: 0.94 - ETA: 15s - loss: 0.1478 - acc: 0.94 - ETA: 11s - loss: 0.1470 - acc: 0.94 - ETA: 7s - loss: 0.1460 - acc: 0.9465 - ETA: 3s - loss: 0.1449 - acc: 0.947 - 102s 9ms/step - loss: 0.1442 - acc: 0.9474 - val_loss: 0.1210 - val_acc: 0.9504\n",
      "Epoch 3/20\n",
      "11702/11702 [==============================] - ETA: 1:29 - loss: 0.1349 - acc: 0.945 - ETA: 1:25 - loss: 0.1475 - acc: 0.937 - ETA: 1:20 - loss: 0.1328 - acc: 0.944 - ETA: 1:16 - loss: 0.1295 - acc: 0.947 - ETA: 1:12 - loss: 0.1307 - acc: 0.947 - ETA: 1:08 - loss: 0.1298 - acc: 0.947 - ETA: 1:04 - loss: 0.1317 - acc: 0.947 - ETA: 1:00 - loss: 0.1309 - acc: 0.947 - ETA: 56s - loss: 0.1322 - acc: 0.947 - ETA: 52s - loss: 0.1319 - acc: 0.94 - ETA: 48s - loss: 0.1323 - acc: 0.94 - ETA: 44s - loss: 0.1312 - acc: 0.94 - ETA: 40s - loss: 0.1319 - acc: 0.94 - ETA: 35s - loss: 0.1306 - acc: 0.94 - ETA: 31s - loss: 0.1312 - acc: 0.94 - ETA: 27s - loss: 0.1305 - acc: 0.94 - ETA: 23s - loss: 0.1291 - acc: 0.94 - ETA: 19s - loss: 0.1294 - acc: 0.94 - ETA: 15s - loss: 0.1310 - acc: 0.94 - ETA: 11s - loss: 0.1311 - acc: 0.94 - ETA: 7s - loss: 0.1315 - acc: 0.9477 - ETA: 3s - loss: 0.1324 - acc: 0.947 - 101s 9ms/step - loss: 0.1324 - acc: 0.9478 - val_loss: 0.1241 - val_acc: 0.9481\n",
      "Epoch 4/20\n",
      " 5632/11702 [=============>................] - ETA: 1:30 - loss: 0.1541 - acc: 0.937 - ETA: 1:25 - loss: 0.1543 - acc: 0.932 - ETA: 1:21 - loss: 0.1418 - acc: 0.941 - ETA: 1:17 - loss: 0.1379 - acc: 0.944 - ETA: 1:13 - loss: 0.1326 - acc: 0.948 - ETA: 1:09 - loss: 0.1265 - acc: 0.952 - ETA: 1:05 - loss: 0.1261 - acc: 0.952 - ETA: 1:01 - loss: 0.1238 - acc: 0.954 - ETA: 57s - loss: 0.1271 - acc: 0.952 - ETA: 53s - loss: 0.1284 - acc: 0.95 - ETA: 48s - loss: 0.1283 - acc: 0.9513"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-03a9c39c13a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             epochs=20)\n\u001b[0m",
      "\u001b[1;32mc:\\users\\joavi\\anaconda3\\envs\\py36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\users\\joavi\\anaconda3\\envs\\py36\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\joavi\\anaconda3\\envs\\py36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\joavi\\anaconda3\\envs\\py36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\joavi\\anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "            x=[X_train_headlines_sequence, X_train_articles_sequence], \n",
    "            y=y_train,\n",
    "            validation_split=0.2,\n",
    "            callbacks=[\n",
    "                ModelCheckpoint(filepath=\"glove100d.hdf5\", monitor='val_loss', save_best_only=True),\n",
    "                ReduceLROnPlateau(patience=1)\n",
    "            ],\n",
    "            verbose=1,\n",
    "            shuffle=True,\n",
    "            batch_size=512,\n",
    "            epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('glove100d.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  : [[0.97004366]]  : 1\n",
      "1000\n",
      "1  : [[0.00095838]]  : 0\n",
      "1000\n",
      "2  : [[0.00177345]]  : 0\n",
      "1000\n",
      "3  : [[0.00381979]]  : 0\n",
      "1000\n",
      "4  : [[0.00222997]]  : 0\n",
      "1000\n",
      "5  : [[0.99620926]]  : 1\n",
      "1000\n",
      "6  : [[0.95734936]]  : 1\n",
      "1000\n",
      "7  : [[0.9788367]]  : 1\n",
      "1000\n",
      "8  : [[0.0073306]]  : 0\n",
      "1000\n",
      "9  : [[0.00380383]]  : 0\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    pred = model.predict([X_test_headlines_sequence[i:i+1] ,X_test_articles_sequence[i:i+1]])\n",
    "    print(i, \" :\", pred, \" :\", y_test[i])\n",
    "    print(len(X_test_articles_sequence[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one photo syrian child caught worlds attention 7 went unnoticed  new york times'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_headlines[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beirut lebanon     omran daqneesh small syrian boy embattled   section aleppo somehow snapped attention millions people around world watched shared arresting video wiped dried blood thick soot face widespread interest    omran surprised doctors treated photographer shot video many syrians wondered whether world discovered children suffered every day war raged five years saturday omrans    brother ali died wounds suffered attack medical workers said alis death draw instant social media outpouring omrans suffering underscored many syrian children dying radar wider world omran injured wednesday either syrian russian airstrike     russia denied involvement     destroyed building family lived eastern aleppo thursday   website published photograph young girl said hurt     around time omran     rebel mortar attacks   western side city rebels air power devastation aleppo greater   side one monitoring group syrian observatory human rights said 100 children died citys eastern side month alone 49 western side family loss immeasurable children constantly caught battles places sides across   syria omrans picture resonated reasons obvious unknowable images seven many children treated past week hospitals region taken among several posted doctors residents aleppo whatsapp group journalists doctors know child arrived hospital treated omran wednesday ahmad separated family     happens many children chaotic aftermath attack     mashhad neighborhood aleppo underwent surgery serious injuries head groin right arm leg later identified ahmad kept intensive care unit hospital along father late friday died injuries hanoun sisters wounded wednesday airstrike injured omran among 12 children 15 treated hospital aleppo girls suffered shrapnel wounds treated released thursday morning doctors shared picture whatsapp group around time shared photograph omran aisel suffered wounds head one legs tuesday treated al quds hospital severity injuries could confirmed doctors busy treating new cases activists nicknamed syrias cinderella picture one took shoes     mary janes worn white socks hayouk siblings suffered cuts bruises aircraft opened fire wednesday sakhour neighborhood treated around time hospital omran childrens wounds relatively minor adult relative suffered critical neck wound efforts identify boy unsuccessful treated tuesday night omar hospital released said baraa   citizen journalist photographed none medical workers could reached remembered boy unusual overwhelmed hospitals 3 saturday barrel bomb landed house jalloum quarter aleppos old city destroying house killing seven members one family     including four children     said abdelkafi   friend fathers children aisha 12 mohammad 11 obaida 7 afraa 6 picture injuries show pulled dead rubble father ali abu joud recorded video three childrens bodies wrapped shrouds voice heard breaking tells goodbye calling habibati     darlings     birds heaven gone one better gone god '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_articles[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "_xh = \"Magnetic Boy Is Probably Just Plump and Sticky Boy\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "_xa = \"Spoons. Frying pans. Industrial-sized irons. The blogosphere has been awash lately with the eclectic mix of objects that stick to a six-year-old Croatian boy’s stomach. In an unfortunately serious story, CBS reported that Magnet boy can carry upwards of 55 pounds of metal on his chubby little frame. What they forget to mention is that the boy’s “magnetic” abilities miraculously extend to mostly non-metal objects too, such as plastic TV remote controls and cell phones. It also doesn’t help that little Ivan Stoiljkovic’s family apparently didn’t think human magnetism was odd enough: They claim that his hands radiate a special kind of heat that allows the boy to soothe “his grandfather’s stomach pains” and “the pain of a neighbor who hurt his leg in a tractor accident.” As for Ivan himself, his cuts apparently heal “very quickly,” leaving no trace of a scar (of course, it probably has nothing to do with the fact that younger skin just heals faster, with its greater elasticity and stronger connective tissues). \\\n",
    "As Nature’s Barbara Ferreira so astutely points out, “If Ivan had indeed magnetic powers, he wouldn’t have the need to bend slightly backwards to keep the items stuck to his body. In fact, he could bend forwards and they wouldn’t fall.” Plus there’s the fact that “the skin on his young, hairless chest is very smooth,” which is a perfect surface to stick smooth objects to. Our bodies are covered with oils that make our skin smooth, and when you put equally smooth objects on already-smooth skin, you’re essentially maximizing the surface area over which chemical bonds form between object and skin. \\\n",
    "There have actually been several “magnetic” boys in the past several years. Towards the beginning of this year, a seven-year-old Serbian boy made headlines for his spoon-sticking abilities. (Apparently this is an especially Balkan phenomenon.) Though you probably won’t make headlines, you can do something similar by breathing on a spoon and balancing it on your nose.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "_xhs = tokenizer.texts_to_sequences([_xh])\n",
    "_xas = tokenizer.texts_to_sequences([_xa])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "_xhp = sequence.pad_sequences(_xhs, maxlen=max_words_headline)\n",
    "_xap = sequence.pad_sequences(_xas, maxlen=max_words_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9476974]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([_xhp, _xap])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
